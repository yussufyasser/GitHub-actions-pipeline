name: Terraform Infra & Ansible MongoDB & Kubernetes Deploy

on:
  workflow_run:
    workflows: ["EKS_Deploy"]
    types:
      - completed
jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      ANSIBLE_HOST_KEY_CHECKING: False
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3


      #Install dependencies
      - name: Install dependencies
        run: |
          sudo apt update
          sudo apt install -y ansible sshpass jq unzip
          ansible-galaxy collection install community.docker

      #Decode SSH key
      - name: Decode SSH key
        run: |
          echo "${{ secrets.EC2_SSH_KEY_B64 }}" | base64 -d > my-ec2-key.pem
          chmod 600 my-ec2-key.pem

      #Start SSH agent and add key
      - name: Start SSH agent and add key
        run: |
          eval "$(ssh-agent -s)"
          ssh-add my-ec2-key.pem

      #get EKS values
      - name: Get EKS values from AWS
        id: eks_values
        run: |
          VPC_ID=$(aws eks describe-cluster --name appcluster --region $AWS_REGION --query "cluster.resourcesVpcConfig.vpcId" --output text)
          PUBLIC_SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=tag:kubernetes.io/role/elb,Values=1" "Name=tag:eksctl.cluster.k8s.io/v1alpha1/cluster-name,Values=appcluster" --region $AWS_REGION --query "Subnets[0].SubnetId" --output text)
          PRIVATE_SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=tag:kubernetes.io/role/internal-elb,Values=1" "Name=tag:eksctl.cluster.k8s.io/v1alpha1/cluster-name,Values=appcluster" --region $AWS_REGION --query "Subnets[0].SubnetId" --output text)
          
          # Get the name of the node group (assuming only one)
          NODEGROUP_NAME=$(aws eks list-nodegroups --cluster-name appcluster --region $AWS_REGION --query "nodegroups[0]" --output text)

          # Get the name of the Auto Scaling Group
          ASG_NAME=$(aws eks describe-nodegroup \
            --cluster-name appcluster \
            --nodegroup-name $NODEGROUP_NAME \
            --region $AWS_REGION \
            --query "nodegroup.resources.autoScalingGroups[0].name" \
            --output text)

          # Get the ID of the first EC2 instance in that ASG
          INSTANCE_ID=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names $ASG_NAME \
            --region $AWS_REGION \
            --query "AutoScalingGroups[0].Instances[0].InstanceId" \
            --output text)

          # Get the Security Group ID of the instance
          SG_ID=$(aws ec2 describe-instances \
            --instance-ids $INSTANCE_ID \
            --region $AWS_REGION \
            --query "Reservations[0].Instances[0].SecurityGroups[0].GroupId" \
            --output text)

          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT
          echo "public_subnet_id=$PUBLIC_SUBNET_ID" >> $GITHUB_OUTPUT
          echo "private_subnet_id=$PRIVATE_SUBNET_ID" >> $GITHUB_OUTPUT
          echo "sg_id=$SG_ID" >> $GITHUB_OUTPUT

      #Terraform Init & Apply
      - name: Terraform Init and Apply
        working-directory: terraform
        run: |
          terraform init
          terraform apply -auto-approve \
            -var="vpc_id=${{ steps.eks_values.outputs.vpc_id }}" \
            -var="public_subnet_id=${{ steps.eks_values.outputs.public_subnet_id }}" \
            -var="private_subnet_id=${{ steps.eks_values.outputs.private_subnet_id }}" \
            -var="eks_worker_sg_id=${{ steps.eks_values.outputs.sg_id }}" \
            -var="key_name=my-ec2-key"

      #Extract Bastion and Private IPs
      - name: Extract Bastion and Private IPs
        working-directory: terraform
        id: tf_outputs
        run: |
          output=$(terraform output -json)
          bastion_ip=$(echo "$output" | jq -r '.bastion_public_ip.value')
          private_ip=$(echo "$output" | jq -r '.private_ec2_private_ip.value')

          echo "BASTION_IP=$bastion_ip" >> $GITHUB_ENV
          echo "PRIVATE_IP=$private_ip" >> $GITHUB_ENV

      #Generate Ansible inventory file
      - name: Generate Ansible inventory file
        run: |
          cat <<EOF > dynamic_inventory.ini
          [mongodb]
          private-instance ansible_host=${{ env.PRIVATE_IP }} ansible_user=ec2-user

          [mongodb:vars]
          ansible_ssh_common_args='-o ProxyCommand="ssh -o StrictHostKeyChecking=no -i my-ec2-key.pem -W %h:%p ec2-user@${{ env.BASTION_IP }}" -o StrictHostKeyChecking=no'
          EOF

      #Run Ansible Playbook
      - name: Run Ansible Playbook
        run: |
          ansible-playbook -i dynamic_inventory.ini ansible/mongodb-playbook.yml --private-key my-ec2-key.pem

      #install kubectl
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.29.0'

      #install helm
      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'
  
      - name: Generate ConfigMap with MongoDB IP
        run: |
          mkdir -p kubernetes/generated
          cat <<EOF | envsubst > kubernetes/generated/configmap.yaml
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: chat-app-config
          data:
            DB_NAME: "chatdb"
            MONGO_HOST: "${PRIVATE_IP}"
            MONGO_PORT: "27017"
          EOF

      #Configure kubeconfig
      - name: Configure kubeconfig
        run: aws eks update-kubeconfig --name appcluster --region $AWS_REGION

      #install helm
      - name: Install Helm
        run: helm upgrade --install chat-app ./mychart

      #get service
      - name: Get Kubernetes services
        run: kubectl get svc
      






      

         
      


        

